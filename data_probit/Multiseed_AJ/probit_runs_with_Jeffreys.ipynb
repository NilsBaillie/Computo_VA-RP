{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fd4d79",
   "metadata": {},
   "source": [
    "# Estimation of paramters in the probit model with an 'exact' Jeffreys\n",
    "\n",
    "This script allows the conduction of *a posteriori* sampling of the paramter in the probit statistical model with a finely approximated Jeffreys prior.\n",
    "\n",
    "\n",
    "The derivation of the Jeffreys prior for the probit model is proposed in the repository [bayes_frag](https://github.com/vbkantoine/bayes_frag). That external code also allows to directly generate samples from the posterior yielded by that approximated Jeffreys prior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c1408",
   "metadata": {},
   "source": [
    "### 1. Clone the bayes_frag github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9b9972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bayes_frag'...\n",
      "remote: Enumerating objects: 74, done.\u001b[K\n",
      "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 74 (delta 32), reused 64 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (74/74), 37.03 KiB | 7.41 MiB/s, done.\n",
      "Resolving deltas: 100% (32/32), done.\n"
     ]
    }
   ],
   "source": [
    "# 1. clone github bayes_frag\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.exists('bayes_frag') :\n",
    "    !git clone \"https://github.com/vbkantoine/bayes_frag.git\"\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),'bayes_frag'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae1ae7",
   "metadata": {},
   "source": [
    "### 2. Compute the Jeffreys prior for this model\n",
    "\n",
    "We recall that the probit model is defined as a parametrized model where $\\theta=(\\theta_1,\\theta_2)\\in(0,\\infty)^2$ is the parameter and the observed variable is $(Z,a)$ where \n",
    "$$\\left\\lbrace\\begin{array}{l}a\\sim\\mathrm{Log-}\\mathcal{N}(\\mu_a,\\sigma_a^2)\\\\\n",
    "Z\\sim\\mathrm{Bernoulli}(P_f(a))\\end{array}\\right.,$$\n",
    "with $P_f(a)=\\Phi\\left(\\frac{\\log a-\\log\\theta_1}{\\theta_2}\\right)$, and $\\Phi$ denoting the c.d.f. of a standard Gaussian.\n",
    "\n",
    "The Jeffreys prior of this model depends on the distribution of $a$, i.e. it depends on $\\mu_a$ and $\\sigma_a$.\n",
    "\n",
    "That external repository proposes a code to compute the Jeffreys prior given a distribution of $a$. Actually, it derives a fine numerical approximation of the Fisher information matrix that is stored in a file called `fisher`.\n",
    "\n",
    "The exection of that code is generally very long given the complex expression of the Fisher information matrix.\n",
    "For this reason, we suggest to download the one that we have already computed and that we provide online on [OSF](https://osf.io/gvqw4/files/osfstorage/678a826e9b2975f377dd6f3f). The dowloaded file `fisher` can be placed at the root of the current directory.\n",
    "\n",
    "We also have provided a lighter approximation of the Fisher information matrix based on a less thin derivation. It is stored in the file called `fisher_light`, that can be renamed by `fisher` to be used. \n",
    "\n",
    "If one wants to do the computations instead of downloading the `fisher` file or renaming the `fisher_light`file, the last line of the following cell must be uncommented. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7d62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. compute and save a fine mesh of Jeffreys prior\n",
    "\n",
    "import util.create_fisher_artificial as cfa\n",
    "\n",
    "dat = cfa.Data_simplified_big()\n",
    "\n",
    "def save_fisher_computation():\n",
    "    save_fisher_arr = cfa.dict_save_fisher['save_fisher_arr'] \n",
    "    # selecting the dictionnary cfa.dict_save_fisher2 above lead to a lighter appproximation of Fisher \n",
    "    # (the one that we have provided and called `fisher_light`)\n",
    "\n",
    "    alpha_min = save_fisher_arr.alpha_min\n",
    "    alpha_max = save_fisher_arr.alpha_max\n",
    "    beta_min = save_fisher_arr.beta_min\n",
    "    beta_max = save_fisher_arr.beta_max\n",
    "    num = save_fisher_arr.num_alpha\n",
    "\n",
    "    theta_tab1 = save_fisher_arr.alpha_tab\n",
    "    theta_tab2 = save_fisher_arr.beta_tab\n",
    "\n",
    "    function = cfa.fisher.fisher_function(\"simpson\", dat)\n",
    "    I = cfa.save_fisher(cfa.save_path, function, theta_tab1, theta_tab2)\n",
    "    \n",
    "# # save_fisher_computation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc86961",
   "metadata": {},
   "source": [
    "### 3. Conduction of *a posteriori* sampling\n",
    "\n",
    "In the following, we import data from the file `../tirages_data` to derive a posterior that is used to generate samples of the paramter $\\theta$.\n",
    "Then, the results are saved on different files according to the prior: Jeffreys or the constrained Jeffreys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db81e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numba import jit\n",
    "\n",
    "from bayes_frag import stat_functions as stat_f\n",
    "from bayes_frag.data import Data\n",
    "from bayes_frag.model import Model\n",
    "from bayes_frag import config\n",
    "from bayes_frag.reference_curves import Reference_known_MLE\n",
    "from bayes_frag.extract_saved_fisher import Approx_fisher\n",
    "\n",
    "from util.create_fisher_artificial import dict_save_fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440fcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_simplified(Data) :\n",
    "    \"\"\"\n",
    "       This class serves the import of data contained in an external file \n",
    "    \"\"\"\n",
    "    def __init__(self, i, pickle_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            i (int): in the generated array, id to take into account in this run\n",
    "        \"\"\"\n",
    "        file = open(pickle_path, mode='rb')\n",
    "        array_A_Z = pickle.load(file)\n",
    "        self.A = np.array(array_A_Z[0][:,i,np.newaxis], dtype=np.float, order='C')\n",
    "        self.Z = np.array(array_A_Z[1][:,i,np.newaxis], dtype=np.float, order=\"C\")\n",
    "        self.Y = np.ones_like(self.A)\n",
    "        self.a_tab = None\n",
    "        self.h_a = None\n",
    "        self._set_a_tab()\n",
    "        self.f_A = None\n",
    "        self.f_A_tab = None\n",
    "        self._compute_f_A()\n",
    "        self.increasing_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba1a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kappa = 1/8\n",
    "alpha = 1/2\n",
    "gamma = kappa/alpha\n",
    "\n",
    "save_folder = './'\n",
    "path_tirages_data = '../tirages_probit/'\n",
    "\n",
    "theta_vrai = np.array([3.37610525, 0.43304097])\n",
    "\n",
    "\n",
    "def experiment(N) :\n",
    "    init_seed = 0\n",
    "    SEED = init_seed + int(N)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    assert not os.path.exists(os.path.join(save_folder, 'model_J_constraint_{}'.format(N-1))), 'existing run no {}'.format(N)\n",
    "\n",
    "    i_data = N-1\n",
    "\n",
    "    data = Data_simplified(i_data, os.path.join(path_tirages_data, 'tirages_data'))\n",
    "\n",
    "    # print(data.A.shape)\n",
    "    # print(data.Z)\n",
    "    error_draw = np.arange(1,41)*5\n",
    "    n_est = 5000\n",
    "\n",
    "    ##\n",
    "    # parameters models\n",
    "    linear = False\n",
    "    approx_fisher_class = Approx_fisher(dict_save_fisher['save_fisher_path'], dict_save_fisher['save_fisher_arr'], fisher_file_path_is_personalized=True)\n",
    "    fisher =  approx_fisher_class.fisher_approx\n",
    "    likelihood = stat_f.log_vrais\n",
    "\n",
    "    iter_HM = 15000\n",
    "    sigma_prop = np.array([[0.1,0],[0,0.095]])\n",
    "    t0 = np.array([3,0.3])\n",
    "    HM_fun = stat_f.adaptative_HM\n",
    "\n",
    "\n",
    "    @jit\n",
    "    def prior(th) :\n",
    "        I = fisher(th)\n",
    "        return 1/2 * np.log(np.nan_to_num(np.abs(I[:,0,0]*I[:,1,1]-I[:,0,1]**2)))\n",
    "\n",
    "    # log_post for this case :\n",
    "    @jit(nopython=True)\n",
    "    def log_post(z,a, theta) :\n",
    "        return stat_f.log_post_jeff_adapt(theta,z,a, Fisher=fisher)\n",
    "\n",
    "    @jit\n",
    "    def prior_adapted(th) :\n",
    "        return prior(th) + gamma * np.log(th[:,1]*th[:,1])\n",
    "\n",
    "    @jit(nopython=True)\n",
    "    def log_post_adapted(z,a,theta) :\n",
    "        return log_post(z,a,theta) + gamma*np.log(theta[:,1])\n",
    "\n",
    "    ref = Reference_known_MLE(data, theta_vrai)\n",
    "    \n",
    "    HM_post_simul_constraint = stat_f.Post_HM_Estimator(HM_fun, t0, log_post_adapted, pi_log=True, max_iter=iter_HM, sigma0=sigma_prop)\n",
    "    model_J_constraint = Model(prior_adapted, likelihood, data, HM_post_simul_constraint, linear=linear, ref=ref)\n",
    "\n",
    "    HM_post_simul = stat_f.Post_HM_Estimator(HM_fun, t0, log_post, pi_log=True, max_iter=iter_HM, sigma0=sigma_prop)\n",
    "    model_J = Model(prior, likelihood, data, HM_post_simul, linear=linear, ref=ref)\n",
    "\n",
    "    \n",
    "    ## run everything\n",
    "\n",
    "    model_J_constraint.run_simulations(error_draw, n_est, sim=['post'], print_fo=True)\n",
    "    model_J.run_simulations(error_draw, n_est, sim=['post'], print_fo=True)\n",
    "\n",
    "    pickle.dump({'logs': model_J_constraint.logs, 'A':model_J_constraint.A, 'S':model_J_constraint.S, 'seed':SEED}, open(os.path.join(save_folder, \"model_J_constraint_{}\".format(i_data)), \"wb\"))\n",
    "    pickle.dump({'logs': model_J.logs, 'A':model_J.A, 'S':model_J.S, 'seed':SEED}, open(os.path.join(save_folder, \"model_J_{}\".format(i_data)), \"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f2ec3",
   "metadata": {},
   "source": [
    "The above function can be ran several time to get samples given different samples of the data.\n",
    "The following permits to conduct $10$ oh these experiments. \n",
    "It re-generates the files `model_J_*` that are provided in the directory.\n",
    "We warn the user that this execution can take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c60b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Experiment no 1/10 ******\n",
      "Simulation 1/40, dataset_size=5, simulating ['post']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.A = np.array(array_A_Z[0][:,i,np.newaxis], dtype=np.float, order='C')\n",
      "/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.Z = np.array(array_A_Z[1][:,i,np.newaxis], dtype=np.float, order=\"C\")\n",
      "/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mCannot cache compiled function \"fisher_approx\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  pi_zv = pi(z_v)\n",
      "/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mCannot cache compiled function \"log_post_jeff_adapt\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  pi_zv = pi(z_v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 2/40, dataset_size=10, simulating ['post']\n",
      "Simulation 3/40, dataset_size=15, simulating ['post']\n",
      "Simulation 4/40, dataset_size=20, simulating ['post']\n",
      "Simulation 5/40, dataset_size=25, simulating ['post']\n",
      "Simulation 6/40, dataset_size=30, simulating ['post']\n",
      "Simulation 7/40, dataset_size=35, simulating ['post']\n",
      "Simulation 8/40, dataset_size=40, simulating ['post']\n",
      "Simulation 9/40, dataset_size=45, simulating ['post']\n",
      "Simulation 10/40, dataset_size=50, simulating ['post']\n",
      "Simulation 11/40, dataset_size=55, simulating ['post']\n",
      "Simulation 12/40, dataset_size=60, simulating ['post']\n",
      "Simulation 13/40, dataset_size=65, simulating ['post']\n",
      "Simulation 14/40, dataset_size=70, simulating ['post']\n",
      "Simulation 15/40, dataset_size=75, simulating ['post']\n",
      "Simulation 16/40, dataset_size=80, simulating ['post']\n",
      "Simulation 17/40, dataset_size=85, simulating ['post']\n",
      "Simulation 18/40, dataset_size=90, simulating ['post']\n",
      "Simulation 19/40, dataset_size=95, simulating ['post']\n",
      "Simulation 20/40, dataset_size=100, simulating ['post']\n",
      "Simulation 21/40, dataset_size=105, simulating ['post']\n",
      "Simulation 22/40, dataset_size=110, simulating ['post']\n",
      "Simulation 23/40, dataset_size=115, simulating ['post']\n",
      "Simulation 24/40, dataset_size=120, simulating ['post']\n",
      "Simulation 25/40, dataset_size=125, simulating ['post']\n",
      "Simulation 26/40, dataset_size=130, simulating ['post']\n",
      "Simulation 27/40, dataset_size=135, simulating ['post']\n",
      "Simulation 28/40, dataset_size=140, simulating ['post']\n",
      "Simulation 29/40, dataset_size=145, simulating ['post']\n",
      "Simulation 30/40, dataset_size=150, simulating ['post']\n",
      "Simulation 31/40, dataset_size=155, simulating ['post']\n",
      "Simulation 32/40, dataset_size=160, simulating ['post']\n",
      "Simulation 33/40, dataset_size=165, simulating ['post']\n",
      "Simulation 34/40, dataset_size=170, simulating ['post']\n",
      "Simulation 35/40, dataset_size=175, simulating ['post']\n",
      "Simulation 36/40, dataset_size=180, simulating ['post']\n",
      "Simulation 37/40, dataset_size=185, simulating ['post']\n",
      "Simulation 38/40, dataset_size=190, simulating ['post']\n",
      "Simulation 39/40, dataset_size=195, simulating ['post']\n",
      "Simulation 40/40, dataset_size=200, simulating ['post']\n",
      "Simulation 1/40, dataset_size=5, simulating ['post']\n",
      "Simulation 2/40, dataset_size=10, simulating ['post']\n",
      "Simulation 3/40, dataset_size=15, simulating ['post']\n",
      "Simulation 4/40, dataset_size=20, simulating ['post']\n",
      "Simulation 5/40, dataset_size=25, simulating ['post']\n",
      "Simulation 6/40, dataset_size=30, simulating ['post']\n",
      "Simulation 7/40, dataset_size=35, simulating ['post']\n",
      "Simulation 8/40, dataset_size=40, simulating ['post']\n",
      "Simulation 9/40, dataset_size=45, simulating ['post']\n",
      "Simulation 10/40, dataset_size=50, simulating ['post']\n",
      "Simulation 11/40, dataset_size=55, simulating ['post']\n",
      "Simulation 12/40, dataset_size=60, simulating ['post']\n",
      "Simulation 13/40, dataset_size=65, simulating ['post']\n",
      "Simulation 14/40, dataset_size=70, simulating ['post']\n",
      "Simulation 15/40, dataset_size=75, simulating ['post']\n",
      "Simulation 16/40, dataset_size=80, simulating ['post']\n",
      "Simulation 17/40, dataset_size=85, simulating ['post']\n",
      "Simulation 18/40, dataset_size=90, simulating ['post']\n",
      "Simulation 19/40, dataset_size=95, simulating ['post']\n",
      "Simulation 20/40, dataset_size=100, simulating ['post']\n",
      "Simulation 21/40, dataset_size=105, simulating ['post']\n",
      "Simulation 22/40, dataset_size=110, simulating ['post']\n",
      "Simulation 23/40, dataset_size=115, simulating ['post']\n",
      "Simulation 24/40, dataset_size=120, simulating ['post']\n",
      "Simulation 25/40, dataset_size=125, simulating ['post']\n",
      "Simulation 26/40, dataset_size=130, simulating ['post']\n",
      "Simulation 27/40, dataset_size=135, simulating ['post']\n",
      "Simulation 28/40, dataset_size=140, simulating ['post']\n",
      "Simulation 29/40, dataset_size=145, simulating ['post']\n",
      "Simulation 30/40, dataset_size=150, simulating ['post']\n",
      "Simulation 31/40, dataset_size=155, simulating ['post']\n",
      "Simulation 32/40, dataset_size=160, simulating ['post']\n",
      "Simulation 33/40, dataset_size=165, simulating ['post']\n",
      "Simulation 34/40, dataset_size=170, simulating ['post']\n",
      "Simulation 35/40, dataset_size=175, simulating ['post']\n",
      "Simulation 36/40, dataset_size=180, simulating ['post']\n",
      "Simulation 37/40, dataset_size=185, simulating ['post']\n",
      "Simulation 38/40, dataset_size=190, simulating ['post']\n",
      "Simulation 39/40, dataset_size=195, simulating ['post']\n",
      "Simulation 40/40, dataset_size=200, simulating ['post']\n",
      "****** Experiment no 2/10 ******\n",
      "Simulation 1/40, dataset_size=5, simulating ['post']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.A = np.array(array_A_Z[0][:,i,np.newaxis], dtype=np.float, order='C')\n",
      "/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.Z = np.array(array_A_Z[1][:,i,np.newaxis], dtype=np.float, order=\"C\")\n",
      "/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mCannot cache compiled function \"fisher_approx\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  pi_zv = pi(z_v)\n",
      "/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mCannot cache compiled function \"log_post_jeff_adapt\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  pi_zv = pi(z_v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 2/40, dataset_size=10, simulating ['post']\n",
      "Simulation 3/40, dataset_size=15, simulating ['post']\n",
      "Simulation 4/40, dataset_size=20, simulating ['post']\n",
      "Simulation 5/40, dataset_size=25, simulating ['post']\n",
      "Simulation 6/40, dataset_size=30, simulating ['post']\n",
      "Simulation 7/40, dataset_size=35, simulating ['post']\n",
      "Simulation 8/40, dataset_size=40, simulating ['post']\n",
      "Simulation 9/40, dataset_size=45, simulating ['post']\n",
      "Simulation 10/40, dataset_size=50, simulating ['post']\n"
     ]
    }
   ],
   "source": [
    "# 4. (long code) all the following run section 3. for 10 different seeds\n",
    "for N in range(1,11) :\n",
    "    print('****** Experiment no {}/10 ******'.format(N))\n",
    "    experiment(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
