[
  {
    "objectID": "data_probit/Multiseed_AJ/probit_runs_with_Jeffreys.html",
    "href": "data_probit/Multiseed_AJ/probit_runs_with_Jeffreys.html",
    "title": "Estimation of paramters in the probit model with an ‘exact’ Jeffreys",
    "section": "",
    "text": "This script allows the conduction of a posteriori sampling of the paramter in the probit statistical model with a finely approximated Jeffreys prior.\nThe derivation of the Jeffreys prior for the probit model is proposed in the repository bayes_frag. That external code also allows to directly generate samples from the posterior yielded by that approximated Jeffreys prior.\n\n1. Clone the bayes_frag github\n\n# 1. clone github bayes_frag\n\nimport os\nimport sys\n\nif not os.path.exists('bayes_frag') :\n    !git clone \"https://github.com/vbkantoine/bayes_frag.git\"\n\nsys.path.append(os.path.join(os.getcwd(),'bayes_frag'))\n\nCloning into 'bayes_frag'...\nremote: Enumerating objects: 74, done.\nremote: Counting objects: 100% (74/74), done.\nremote: Compressing objects: 100% (43/43), done.\nremote: Total 74 (delta 32), reused 64 (delta 22), pack-reused 0 (from 0)\nReceiving objects: 100% (74/74), 37.03 KiB | 7.41 MiB/s, done.\nResolving deltas: 100% (32/32), done.\n\n\n\n\n2. Compute the Jeffreys prior for this model\nWe recall that the probit model is defined as a parametrized model where \\(\\theta=(\\theta_1,\\theta_2)\\in(0,\\infty)^2\\) is the parameter and the observed variable is \\((Z,a)\\) where \\[\\left\\lbrace\\begin{array}{l}a\\sim\\mathrm{Log-}\\mathcal{N}(\\mu_a,\\sigma_a^2)\\\\\nZ\\sim\\mathrm{Bernoulli}(P_f(a))\\end{array}\\right.,\\] with \\(P_f(a)=\\Phi\\left(\\frac{\\log a-\\log\\theta_1}{\\theta_2}\\right)\\), and \\(\\Phi\\) denoting the c.d.f. of a standard Gaussian.\nThe Jeffreys prior of this model depends on the distribution of \\(a\\), i.e. it depends on \\(\\mu_a\\) and \\(\\sigma_a\\).\nThat external repository proposes a code to compute the Jeffreys prior given a distribution of \\(a\\). Actually, it derives a fine numerical approximation of the Fisher information matrix that is stored in a file called fisher.\nThe exection of that code is generally very long given the complex expression of the Fisher information matrix. For this reason, we suggest to download the one that we have already computed and that we provide online on OSF. The dowloaded file fisher can be placed at the root of the current directory.\nWe also have provided a lighter approximation of the Fisher information matrix based on a less thin derivation. It is stored in the file called fisher_light, that can be renamed by fisher to be used.\nIf one wants to do the computations instead of downloading the fisher file or renaming the fisher_lightfile, the last line of the following cell must be uncommented.\n\n# 2. compute and save a fine mesh of Jeffreys prior\n\nimport util.create_fisher_artificial as cfa\n\ndat = cfa.Data_simplified_big()\n\ndef save_fisher_computation():\n    save_fisher_arr = cfa.dict_save_fisher['save_fisher_arr'] \n    # selecting the dictionnary cfa.dict_save_fisher2 above lead to a lighter appproximation of Fisher \n    # (the one that we have provided and called `fisher_light`)\n\n    alpha_min = save_fisher_arr.alpha_min\n    alpha_max = save_fisher_arr.alpha_max\n    beta_min = save_fisher_arr.beta_min\n    beta_max = save_fisher_arr.beta_max\n    num = save_fisher_arr.num_alpha\n\n    theta_tab1 = save_fisher_arr.alpha_tab\n    theta_tab2 = save_fisher_arr.beta_tab\n\n    function = cfa.fisher.fisher_function(\"simpson\", dat)\n    I = cfa.save_fisher(cfa.save_path, function, theta_tab1, theta_tab2)\n    \n# # save_fisher_computation()\n\n\n\n3. Conduction of a posteriori sampling\nIn the following, we import data from the file ../tirages_data to derive a posterior that is used to generate samples of the paramter \\(\\theta\\). Then, the results are saved on different files according to the prior: Jeffreys or the constrained Jeffreys.\n\nimport os\nimport numpy as np\nimport pickle\nfrom numba import jit\n\nfrom bayes_frag import stat_functions as stat_f\nfrom bayes_frag.data import Data\nfrom bayes_frag.model import Model\nfrom bayes_frag import config\nfrom bayes_frag.reference_curves import Reference_known_MLE\nfrom bayes_frag.extract_saved_fisher import Approx_fisher\n\nfrom util.create_fisher_artificial import dict_save_fisher\n\n\nclass Data_simplified(Data) :\n    \"\"\"\n       This class serves the import of data contained in an external file \n    \"\"\"\n    def __init__(self, i, pickle_path):\n        \"\"\"\n        Args:\n            i (int): in the generated array, id to take into account in this run\n        \"\"\"\n        file = open(pickle_path, mode='rb')\n        array_A_Z = pickle.load(file)\n        self.A = np.array(array_A_Z[0][:,i,np.newaxis], dtype=np.float, order='C')\n        self.Z = np.array(array_A_Z[1][:,i,np.newaxis], dtype=np.float, order=\"C\")\n        self.Y = np.ones_like(self.A)\n        self.a_tab = None\n        self.h_a = None\n        self._set_a_tab()\n        self.f_A = None\n        self.f_A_tab = None\n        self._compute_f_A()\n        self.increasing_mode = True\n\n\n\nkappa = 1/8\nalpha = 1/2\ngamma = kappa/alpha\n\nsave_folder = './'\npath_tirages_data = '../tirages_probit/'\n\ntheta_vrai = np.array([3.37610525, 0.43304097])\n\n\ndef experiment(N) :\n    init_seed = 0\n    SEED = init_seed + int(N)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    assert not os.path.exists(os.path.join(save_folder, 'model_J_constraint_{}'.format(N-1))), 'existing run no {}'.format(N)\n\n    i_data = N-1\n\n    data = Data_simplified(i_data, os.path.join(path_tirages_data, 'tirages_data'))\n\n    # print(data.A.shape)\n    # print(data.Z)\n    error_draw = np.arange(1,41)*5\n    n_est = 5000\n\n    ##\n    # parameters models\n    linear = False\n    approx_fisher_class = Approx_fisher(dict_save_fisher['save_fisher_path'], dict_save_fisher['save_fisher_arr'], fisher_file_path_is_personalized=True)\n    fisher =  approx_fisher_class.fisher_approx\n    likelihood = stat_f.log_vrais\n\n    iter_HM = 15000\n    sigma_prop = np.array([[0.1,0],[0,0.095]])\n    t0 = np.array([3,0.3])\n    HM_fun = stat_f.adaptative_HM\n\n\n    @jit\n    def prior(th) :\n        I = fisher(th)\n        return 1/2 * np.log(np.nan_to_num(np.abs(I[:,0,0]*I[:,1,1]-I[:,0,1]**2)))\n\n    # log_post for this case :\n    @jit(nopython=True)\n    def log_post(z,a, theta) :\n        return stat_f.log_post_jeff_adapt(theta,z,a, Fisher=fisher)\n\n    @jit\n    def prior_adapted(th) :\n        return prior(th) + gamma * np.log(th[:,1]*th[:,1])\n\n    @jit(nopython=True)\n    def log_post_adapted(z,a,theta) :\n        return log_post(z,a,theta) + gamma*np.log(theta[:,1])\n\n    ref = Reference_known_MLE(data, theta_vrai)\n    \n    HM_post_simul_constraint = stat_f.Post_HM_Estimator(HM_fun, t0, log_post_adapted, pi_log=True, max_iter=iter_HM, sigma0=sigma_prop)\n    model_J_constraint = Model(prior_adapted, likelihood, data, HM_post_simul_constraint, linear=linear, ref=ref)\n\n    HM_post_simul = stat_f.Post_HM_Estimator(HM_fun, t0, log_post, pi_log=True, max_iter=iter_HM, sigma0=sigma_prop)\n    model_J = Model(prior, likelihood, data, HM_post_simul, linear=linear, ref=ref)\n\n    \n    ## run everything\n\n    model_J_constraint.run_simulations(error_draw, n_est, sim=['post'], print_fo=True)\n    model_J.run_simulations(error_draw, n_est, sim=['post'], print_fo=True)\n\n    pickle.dump({'logs': model_J_constraint.logs, 'A':model_J_constraint.A, 'S':model_J_constraint.S, 'seed':SEED}, open(os.path.join(save_folder, \"model_J_constraint_{}\".format(i_data)), \"wb\"))\n    pickle.dump({'logs': model_J.logs, 'A':model_J.A, 'S':model_J.S, 'seed':SEED}, open(os.path.join(save_folder, \"model_J_{}\".format(i_data)), \"wb\"))\n\n\n\nThe above function can be ran several time to get samples given different samples of the data. The following permits to conduct \\(10\\) oh these experiments. It re-generates the files model_J_* that are provided in the directory. We warn the user that this execution can take a long time.\n\n# 4. (long code) all the following run section 3. for 10 different seeds\nfor N in range(1,11) :\n    print('****** Experiment no {}/10 ******'.format(N))\n    experiment(N)\n\n****** Experiment no 1/10 ******\nSimulation 1/40, dataset_size=5, simulating ['post']\n\n\n/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  self.A = np.array(array_A_Z[0][:,i,np.newaxis], dtype=np.float, order='C')\n/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  self.Z = np.array(array_A_Z[1][:,i,np.newaxis], dtype=np.float, order=\"C\")\n/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: Cannot cache compiled function \"fisher_approx\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\n  pi_zv = pi(z_v)\n/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: Cannot cache compiled function \"log_post_jeff_adapt\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\n  pi_zv = pi(z_v)\n\n\nSimulation 2/40, dataset_size=10, simulating ['post']\nSimulation 3/40, dataset_size=15, simulating ['post']\nSimulation 4/40, dataset_size=20, simulating ['post']\nSimulation 5/40, dataset_size=25, simulating ['post']\nSimulation 6/40, dataset_size=30, simulating ['post']\nSimulation 7/40, dataset_size=35, simulating ['post']\nSimulation 8/40, dataset_size=40, simulating ['post']\nSimulation 9/40, dataset_size=45, simulating ['post']\nSimulation 10/40, dataset_size=50, simulating ['post']\nSimulation 11/40, dataset_size=55, simulating ['post']\nSimulation 12/40, dataset_size=60, simulating ['post']\nSimulation 13/40, dataset_size=65, simulating ['post']\nSimulation 14/40, dataset_size=70, simulating ['post']\nSimulation 15/40, dataset_size=75, simulating ['post']\nSimulation 16/40, dataset_size=80, simulating ['post']\nSimulation 17/40, dataset_size=85, simulating ['post']\nSimulation 18/40, dataset_size=90, simulating ['post']\nSimulation 19/40, dataset_size=95, simulating ['post']\nSimulation 20/40, dataset_size=100, simulating ['post']\nSimulation 21/40, dataset_size=105, simulating ['post']\nSimulation 22/40, dataset_size=110, simulating ['post']\nSimulation 23/40, dataset_size=115, simulating ['post']\nSimulation 24/40, dataset_size=120, simulating ['post']\nSimulation 25/40, dataset_size=125, simulating ['post']\nSimulation 26/40, dataset_size=130, simulating ['post']\nSimulation 27/40, dataset_size=135, simulating ['post']\nSimulation 28/40, dataset_size=140, simulating ['post']\nSimulation 29/40, dataset_size=145, simulating ['post']\nSimulation 30/40, dataset_size=150, simulating ['post']\nSimulation 31/40, dataset_size=155, simulating ['post']\nSimulation 32/40, dataset_size=160, simulating ['post']\nSimulation 33/40, dataset_size=165, simulating ['post']\nSimulation 34/40, dataset_size=170, simulating ['post']\nSimulation 35/40, dataset_size=175, simulating ['post']\nSimulation 36/40, dataset_size=180, simulating ['post']\nSimulation 37/40, dataset_size=185, simulating ['post']\nSimulation 38/40, dataset_size=190, simulating ['post']\nSimulation 39/40, dataset_size=195, simulating ['post']\nSimulation 40/40, dataset_size=200, simulating ['post']\nSimulation 1/40, dataset_size=5, simulating ['post']\nSimulation 2/40, dataset_size=10, simulating ['post']\nSimulation 3/40, dataset_size=15, simulating ['post']\nSimulation 4/40, dataset_size=20, simulating ['post']\nSimulation 5/40, dataset_size=25, simulating ['post']\nSimulation 6/40, dataset_size=30, simulating ['post']\nSimulation 7/40, dataset_size=35, simulating ['post']\nSimulation 8/40, dataset_size=40, simulating ['post']\nSimulation 9/40, dataset_size=45, simulating ['post']\nSimulation 10/40, dataset_size=50, simulating ['post']\nSimulation 11/40, dataset_size=55, simulating ['post']\nSimulation 12/40, dataset_size=60, simulating ['post']\nSimulation 13/40, dataset_size=65, simulating ['post']\nSimulation 14/40, dataset_size=70, simulating ['post']\nSimulation 15/40, dataset_size=75, simulating ['post']\nSimulation 16/40, dataset_size=80, simulating ['post']\nSimulation 17/40, dataset_size=85, simulating ['post']\nSimulation 18/40, dataset_size=90, simulating ['post']\nSimulation 19/40, dataset_size=95, simulating ['post']\nSimulation 20/40, dataset_size=100, simulating ['post']\nSimulation 21/40, dataset_size=105, simulating ['post']\nSimulation 22/40, dataset_size=110, simulating ['post']\nSimulation 23/40, dataset_size=115, simulating ['post']\nSimulation 24/40, dataset_size=120, simulating ['post']\nSimulation 25/40, dataset_size=125, simulating ['post']\nSimulation 26/40, dataset_size=130, simulating ['post']\nSimulation 27/40, dataset_size=135, simulating ['post']\nSimulation 28/40, dataset_size=140, simulating ['post']\nSimulation 29/40, dataset_size=145, simulating ['post']\nSimulation 30/40, dataset_size=150, simulating ['post']\nSimulation 31/40, dataset_size=155, simulating ['post']\nSimulation 32/40, dataset_size=160, simulating ['post']\nSimulation 33/40, dataset_size=165, simulating ['post']\nSimulation 34/40, dataset_size=170, simulating ['post']\nSimulation 35/40, dataset_size=175, simulating ['post']\nSimulation 36/40, dataset_size=180, simulating ['post']\nSimulation 37/40, dataset_size=185, simulating ['post']\nSimulation 38/40, dataset_size=190, simulating ['post']\nSimulation 39/40, dataset_size=195, simulating ['post']\nSimulation 40/40, dataset_size=200, simulating ['post']\n****** Experiment no 2/10 ******\nSimulation 1/40, dataset_size=5, simulating ['post']\n\n\n/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  self.A = np.array(array_A_Z[0][:,i,np.newaxis], dtype=np.float, order='C')\n/var/folders/q2/pg0k5xqs0r9brvk6cxh1hb_m0000gn/T/ipykernel_16765/3424857049.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  self.Z = np.array(array_A_Z[1][:,i,np.newaxis], dtype=np.float, order=\"C\")\n/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: Cannot cache compiled function \"fisher_approx\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\n  pi_zv = pi(z_v)\n/Users/antoinevanbiesbroeck/Documents/code computo merged/bayes_frag/bayes_frag/stat_functions.py:213: NumbaWarning: Cannot cache compiled function \"log_post_jeff_adapt\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\n  pi_zv = pi(z_v)\n\n\nSimulation 2/40, dataset_size=10, simulating ['post']\nSimulation 3/40, dataset_size=15, simulating ['post']\nSimulation 4/40, dataset_size=20, simulating ['post']\nSimulation 5/40, dataset_size=25, simulating ['post']\nSimulation 6/40, dataset_size=30, simulating ['post']\nSimulation 7/40, dataset_size=35, simulating ['post']\nSimulation 8/40, dataset_size=40, simulating ['post']\nSimulation 9/40, dataset_size=45, simulating ['post']\nSimulation 10/40, dataset_size=50, simulating ['post']"
  }
]